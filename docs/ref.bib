@article{arxiv:FastRCNN,
  author    = {Ross B. Girshick},
  title     = {Fast {R-CNN}},
  journal   = {CoRR},
  volume    = {abs/1504.08083},
  year      = {2015},
  url       = {http://arxiv.org/abs/1504.08083},
  eprinttype = {arXiv},
  eprint    = {1504.08083},
  timestamp = {Mon, 13 Aug 2018 16:49:11 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/Girshick15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{arxiv:FasterRCNN,
  author    = {Shaoqing Ren and
               Kaiming He and
               Ross B. Girshick and
               Jian Sun},
  title     = {Faster {R-CNN:} Towards Real-Time Object Detection with Region Proposal
               Networks},
  journal   = {CoRR},
  volume    = {abs/1506.01497},
  year      = {2015},
  url       = {http://arxiv.org/abs/1506.01497},
  eprinttype = {arXiv},
  eprint    = {1506.01497},
  timestamp = {Mon, 13 Aug 2018 16:46:02 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/RenHG015.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Lowe:SIFT,
  abstract = {This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.},
  acmid = {996342},
  added-at = {2012-11-08T15:54:11.000+0100},
  address = {Hingham, MA, USA},
  author = {Lowe, David G.},
  biburl = {https://www.bibsonomy.org/bibtex/2c9984d3a783a48553018a518847f6657/daill},
  description = {Distinctive Image Features from Scale-Invariant Keypoints},
  doi = {10.1023/B:VISI.0000029664.99615.94},
  interhash = {a1c2b94c96ee2ef15ef53e73b7fd9a8d},
  intrahash = {c9984d3a783a48553018a518847f6657},
  issn = {0920-5691},
  issue_date = {November 2004},
  journal = {Int. J. Comput. Vision},
  keywords = {feature sift},
  month = nov,
  number = 2,
  numpages = {20},
  pages = {91--110},
  publisher = {Kluwer Academic Publishers},
  timestamp = {2012-11-08T15:54:11.000+0100},
  title = {Distinctive Image Features from Scale-Invariant Keypoints},
  url = {http://dx.doi.org/10.1023/B:VISI.0000029664.99615.94},
  volume = 60,
  year = 2004
}

@article{Bay:SURF,
  abstract = {This article presents a novel scale- and rotation-invariant detector and descriptor, coined SURF (Speeded-Up Robust Features). SURF approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster.

This is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descriptors (specifically, using a Hessian matrix-based measure for the detector, and a distribution-based descriptor); and by simplifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps.

The paper encompasses a detailed description of the detector and descriptor and then explores the effects of the most important parameters. We conclude the article with SURF’s application to two challenging, yet converse goals: camera calibration as a special case of image registration, and object recognition. Our experiments underline SURF’s usefulness in a broad range of topics in computer vision.},
  added-at = {2012-09-30T19:58:26.000+0200},
  author = {Bay, Herbert and Ess, Andreas and Tuytelaars, Tinne and Gool, Luc Van},
  biburl = {https://www.bibsonomy.org/bibtex/2dfea172dfaca0272dcdc66acf92ec58d/daill},
  description = {ScienceDirect.com - Computer Vision and Image Understanding - Speeded-Up Robust Features (SURF)},
  doi = {10.1016/j.cviu.2007.09.014},
  interhash = {befc128dd8bce45a9d0308f72dc9a95a},
  intrahash = {dfea172dfaca0272dcdc66acf92ec58d},
  issn = {1077-3142},
  journal = {Computer Vision and Image Understanding},
  keywords = {detector feature surf},
  note = {Similarity Matching in Computer Vision and Multimedia},
  number = 3,
  pages = {346 - 359},
  timestamp = {2012-09-30T19:58:26.000+0200},
  title = {Speeded-Up Robust Features (SURF)},
  url = {http://www.sciencedirect.com/science/article/pii/S1077314207001555},
  volume = 110,
  year = 2008
}

@InProceedings{Chen:FCNN,
author = {Chen, Qifeng and Xu, Jia and Koltun, Vladlen},
title = {Fast Image Processing With Fully-Convolutional Networks},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {Oct},
year = {2017}
}

@misc{Simonyan:VCC16,
  doi = {10.48550/ARXIV.1409.1556},
  url = {https://arxiv.org/abs/1409.1556},
  author = {Simonyan, Karen and Zisserman, Andrew},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  publisher = {arXiv},
  year = {2014},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{Neubeck:NMS,
  title={Efficient non-maximum suppression},
  author={Neubeck, Alexander and Van Gool, Luc},
  booktitle={18th International Conference on Pattern Recognition (ICPR'06)},
  volume={3},
  pages={850--855},
  year={2006},
  organization={IEEE}
}

@misc{Rezatofighi:IoU,
  doi = {10.48550/ARXIV.1902.09630},
  url = {https://arxiv.org/abs/1902.09630},
  author = {Rezatofighi, Hamid and Tsoi, Nathan and Gwak, JunYoung and Sadeghian, Amir and Reid, Ian and Savarese, Silvio},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression},
  publisher = {arXiv},
  year = {2019},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@misc{Mojang-Minecraft,
  title        = "Minecraft",
  booktitle    = "Minecraft.net",
  author       = "{Mojang}",
  publisher    = "Microsoft",
  month        =  nov,
  year         =  2011,
  howpublished = "\url{https://www.minecraft.net/en-us}",
}

@article{Chechik:Triplet,
author = {Chechik, Gal and Sharma, Varun and Shalit, Uri and Bengio, Samy},
title = {Large Scale Online Learning of Image Similarity Through Ranking},
year = {2010},
issue_date = {3/1/2010},
publisher = {JMLR.org},
volume = {11},
issn = {1532-4435},
abstract = {Learning a measure of similarity between pairs of objects is an important generic problem in machine learning. It is particularly useful in large scale applications like searching for an image that is similar to a given image or finding videos that are relevant to a given video. In these tasks, users look for objects that are not only visually similar but also semantically related to a given object. Unfortunately, the approaches that exist today for learning such semantic similarity do not scale to large data sets. This is both because typically their CPU and storage requirements grow quadratically with the sample size, and because many methods impose complex positivity constraints on the space of learned similarity functions.The current paper presents OASIS, an Online Algorithm for Scalable Image Similarity learning that learns a bilinear similarity measure over sparse representations. OASIS is an online dual approach using the passive-aggressive family of learning algorithms with a large margin criterion and an efficient hinge loss cost. Our experiments show that OASIS is both fast and accurate at a wide range of scales: for a data set with thousands of images, it achieves better results than existing state-of-the-art methods, while being an order of magnitude faster. For large, web scale, data sets, OASIS can be trained on more than two million images from 150K text queries within 3 days on a single CPU. On this large scale data set, human evaluations showed that 35% of the ten nearest neighbors of a given test image, as found by OASIS, were semantically relevant to that image. This suggests that query independent similarity could be accurately learned even for large scale data sets that could not be handled before.},
journal = {J. Mach. Learn. Res.},
month = {mar},
pages = {1109–1135},
numpages = {27}
}

@article{Srivastava:Dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}

@inproceedings{arxiv:Kaiming,
  title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1026--1034},
  year={2015}
}